{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "## Modelling (BaseLine)\n",
    "## Modelling (Testing)\n",
    "## Modelling (Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_size=112):\n",
    "    data = []\n",
    "    index = -1\n",
    "    labels = {}\n",
    "    for directory in os.listdir('./data/')[:37]:\n",
    "        index += 1\n",
    "        labels[f'./data/{directory}/'] = [index,-1]\n",
    "    for label in tqdm(labels):\n",
    "        for file in os.listdir(label):\n",
    "            filepath = label + file\n",
    "            img = cv2.imread(filepath)\n",
    "            img = cv2.resize(img,(img_size,img_size))\n",
    "#             img = img / 255.0\n",
    "            data.append([\n",
    "                np.array(img),\n",
    "                labels[label][0]\n",
    "            ])\n",
    "            labels[label][1] += 1\n",
    "    for _ in range(12):\n",
    "        np.random.shuffle(data)\n",
    "    print(len(data))\n",
    "#     np.save('./data.npy',data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_loading_data_proccess(data):\n",
    "    print(len(data))\n",
    "    X = []\n",
    "    y = []\n",
    "    print('going through the data..')\n",
    "    for d in data:\n",
    "        X.append(d[0])\n",
    "        y.append(d[1])\n",
    "    print('splitting the data')\n",
    "    VAL_SPLIT = 0.25\n",
    "    VAL_SPLIT = len(X)*VAL_SPLIT\n",
    "    VAL_SPLIT = int(VAL_SPLIT)\n",
    "    X_train = X[:-VAL_SPLIT]\n",
    "    y_train = y[:-VAL_SPLIT]\n",
    "    X_test = X[-VAL_SPLIT:]\n",
    "    y_test = y[-VAL_SPLIT:]\n",
    "    print('turning data to tensors')\n",
    "    X_train = torch.from_numpy(np.array(X_train))\n",
    "    print(len(X_train))\n",
    "    y_train = torch.from_numpy(np.array(y_train))\n",
    "    print(len(y_train))\n",
    "    X_test = torch.from_numpy(np.array(X_test))\n",
    "    print(len(X_test))\n",
    "    y_test = torch.from_numpy(np.array(y_test))\n",
    "    print(len(y_test))\n",
    "    return [X_train,X_test,y_train,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:03<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18895\n",
      "18895\n",
      "going through the data..\n",
      "splitting the data\n",
      "turning data to tensors\n",
      "14172\n",
      "14172\n",
      "4723\n",
      "4723\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = True\n",
    "if REBUILD_DATA:\n",
    "    data = load_data()\n",
    "    np.random.shuffle(data)\n",
    "    X_train,X_test,y_train,y_test = other_loading_data_proccess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.CenterCrop(112),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.25,0.25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(X_train)):\n",
    "    testing = X_train[index]\n",
    "    testing = np.array(testing)\n",
    "    testing = Image.fromarray(testing)\n",
    "    X_train_new.append(np.array(transform_train(testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.array(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train_new.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14172, 3, 112, 112])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True).to(device)\n",
    "model = model.to(device)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 37)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:10<00:00, 10.88s/it]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(EPOCHS)):\n",
    "    for i in range(0,len(X_train),BATCH_SIZE):\n",
    "        X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "        y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "        model = model.to(device)\n",
    "        preds = model(X_batch.float().to(device))\n",
    "        preds = preds.to(device)\n",
    "        loss = criterion(preds,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_logs.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9d0407a908>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 37])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in preds:\n",
    "    preds_new.append(int(torch.argmax(torch.round(pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(criterion,y,model,X):\n",
    "    model.to('cpu')\n",
    "    preds = model(X.view(-1,1,112,112).to('cpu').float())\n",
    "    preds.to('cpu')\n",
    "    loss = criterion(preds,torch.tensor(y,dtype=torch.long).to('cpu'))\n",
    "    loss.backward()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,X,y):\n",
    "    device = 'cpu'\n",
    "    net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            real_class = torch.argmax(y[i]).to(device)\n",
    "            net_out = net(X[i].view(-1,1,112,112).to(device).float())\n",
    "            net_out = net_out[0]\n",
    "            predictied_class = torch.argmax(net_out)\n",
    "            if predictied_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    net.train()\n",
    "    net.to('cuda')\n",
    "    return round(correct/total,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'fruits-37'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">optimizer-Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.1\n",
       "    weight_decay: 0\n",
       ")</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ranuga-d/fruits-37\" target=\"_blank\">https://wandb.ai/ranuga-d/fruits-37</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ranuga-d/fruits-37/runs/21jz7o5z\" target=\"_blank\">https://wandb.ai/ranuga-d/fruits-37/runs/21jz7o5z</a><br/>\n",
       "                Run data is saved locally in <code>/home/indika/Programming/Projects/Python/Artifical-Intelligence/PyTorch/CNN/Fruits-360/wandb/run-20210521_010952-21jz7o5z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "optimizers = [torch.optim.Adam,torch.optim.AdamW,torch.optim.Adamax,torch.optim.SGD]\n",
    "for optimizer in optimizers:\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "    model = model.to(device)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 37)\n",
    "    optimizer = optimizer(model.parameters(),lr=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    wandb.init(project=PROJECT_NAME,name=f'optimizer-{optimizer}')\n",
    "    for _ in tqdm(range(EPOCHS)):\n",
    "        for i in range(0,len(X_train),BATCH_SIZE):\n",
    "            X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "            y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "            model = model.to(device)\n",
    "            preds = model(X_batch.float().to(device))\n",
    "            preds = preds.to(device)\n",
    "            loss = criterion(preds,y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        wandb.log({'loss':loss.item(),'accuracy':test(model,X_train,y_train),'val_accuracy':test(model,X_test,y_test),'val_loss':get_loss(criterion,y_test,model,X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
